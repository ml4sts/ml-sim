Traceback (most recent call last):
  File "/opt/hostedtoolcache/Python/3.8.15/x64/lib/python3.8/site-packages/jupyter_cache/executors/utils.py", line 51, in single_nb_execution
    executenb(
  File "/opt/hostedtoolcache/Python/3.8.15/x64/lib/python3.8/site-packages/nbclient/client.py", line 1204, in execute
    return NotebookClient(nb=nb, resources=resources, km=km, **kwargs).execute()
  File "/opt/hostedtoolcache/Python/3.8.15/x64/lib/python3.8/site-packages/nbclient/util.py", line 84, in wrapped
    return just_run(coro(*args, **kwargs))
  File "/opt/hostedtoolcache/Python/3.8.15/x64/lib/python3.8/site-packages/nbclient/util.py", line 62, in just_run
    return loop.run_until_complete(coro)
  File "/opt/hostedtoolcache/Python/3.8.15/x64/lib/python3.8/asyncio/base_events.py", line 616, in run_until_complete
    return future.result()
  File "/opt/hostedtoolcache/Python/3.8.15/x64/lib/python3.8/site-packages/nbclient/client.py", line 663, in async_execute
    await self.async_execute_cell(
  File "/opt/hostedtoolcache/Python/3.8.15/x64/lib/python3.8/site-packages/nbclient/client.py", line 965, in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
  File "/opt/hostedtoolcache/Python/3.8.15/x64/lib/python3.8/site-packages/nbclient/client.py", line 862, in _check_raise_for_error
    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
------------------
import mlsim
import pandas as pd
import numpy as np
import seaborn as sns
from collections import namedtuple
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mImportError[0m                               Traceback (most recent call last)
Cell [0;32mIn[1], line 1[0m
[0;32m----> 1[0m [38;5;28;01mimport[39;00m [38;5;21;01mmlsim[39;00m
[1;32m      2[0m [38;5;28;01mimport[39;00m [38;5;21;01mpandas[39;00m [38;5;28;01mas[39;00m [38;5;21;01mpd[39;00m
[1;32m      3[0m [38;5;28;01mimport[39;00m [38;5;21;01mnumpy[39;00m [38;5;28;01mas[39;00m [38;5;21;01mnp[39;00m

File [0;32m~/work/ml-sim/ml-sim/mlsim/__init__.py:5[0m
[1;32m      3[0m [38;5;28;01mfrom[39;00m [38;5;21;01m.[39;00m [38;5;28;01mimport[39;00m anomaly
[0;32m----> 5[0m [38;5;28;01mfrom[39;00m [38;5;21;01m.[39;00m [38;5;28;01mimport[39;00m bias
[1;32m      7[0m __all__ [38;5;241m=[39m [[38;5;124m'[39m[38;5;124mbias[39m[38;5;124m'[39m,[38;5;124m'[39m[38;5;124manomaly[39m[38;5;124m'[39m]

File [0;32m~/work/ml-sim/ml-sim/mlsim/bias/__init__.py:1[0m
[0;32m----> 1[0m [38;5;28;01mfrom[39;00m [38;5;21;01m.[39;00m[38;5;21;01mpopulations[39;00m [38;5;28;01mimport[39;00m Population, PopulationInstantiated
[1;32m      2[0m [38;5;28;01mfrom[39;00m [38;5;21;01m.[39;00m[38;5;21;01mbias_components[39;00m [38;5;28;01mimport[39;00m Demographic, DemographicIndependent, DemographicCorrelated
[1;32m      3[0m [38;5;28;01mfrom[39;00m [38;5;21;01m.[39;00m[38;5;21;01mbias_components[39;00m [38;5;28;01mimport[39;00m Target, TargetDisadvantagedError, TargetTwoError

File [0;32m~/work/ml-sim/ml-sim/mlsim/bias/populations.py:3[0m
[1;32m      1[0m [38;5;28;01mimport[39;00m [38;5;21;01mnumpy[39;00m [38;5;28;01mas[39;00m [38;5;21;01mnp[39;00m
[1;32m      2[0m [38;5;28;01mimport[39;00m [38;5;21;01mpandas[39;00m [38;5;28;01mas[39;00m [38;5;21;01mpd[39;00m
[0;32m----> 3[0m [38;5;28;01mimport[39;00m [38;5;21;01maif360[39;00m[38;5;21;01m.[39;00m[38;5;21;01mdatasets[39;00m
[1;32m      4[0m [38;5;28;01mfrom[39;00m [38;5;21;01m.[39;00m[38;5;21;01mbias_components[39;00m [38;5;28;01mimport[39;00m Demographic, Target, Feature, FeatureNoise
[1;32m      6[0m default_params [38;5;241m=[39m {[38;5;124m'[39m[38;5;124mdem[39m[38;5;124m'[39m:[38;5;28;01mNone[39;00m,}

File [0;32m/opt/hostedtoolcache/Python/3.8.15/x64/lib/python3.8/site-packages/aif360/datasets/__init__.py:13[0m
[1;32m     11[0m [38;5;28;01mfrom[39;00m [38;5;21;01maif360[39;00m[38;5;21;01m.[39;00m[38;5;21;01mdatasets[39;00m[38;5;21;01m.[39;00m[38;5;21;01mmeps_dataset_panel21_fy2016[39;00m [38;5;28;01mimport[39;00m MEPSDataset21
[1;32m     12[0m [38;5;28;01mfrom[39;00m [38;5;21;01maif360[39;00m[38;5;21;01m.[39;00m[38;5;21;01mdatasets[39;00m[38;5;21;01m.[39;00m[38;5;21;01mregression_dataset[39;00m [38;5;28;01mimport[39;00m RegressionDataset
[0;32m---> 13[0m [38;5;28;01mfrom[39;00m [38;5;21;01maif360[39;00m[38;5;21;01m.[39;00m[38;5;21;01mdatasets[39;00m[38;5;21;01m.[39;00m[38;5;21;01mlaw_school_gpa_dataset[39;00m [38;5;28;01mimport[39;00m LawSchoolGPADataset 

File [0;32m/opt/hostedtoolcache/Python/3.8.15/x64/lib/python3.8/site-packages/aif360/datasets/law_school_gpa_dataset.py:4[0m
[1;32m      2[0m [38;5;28;01mimport[39;00m [38;5;21;01mpandas[39;00m [38;5;28;01mas[39;00m [38;5;21;01mpd[39;00m
[1;32m      3[0m [38;5;28;01mfrom[39;00m [38;5;21;01maif360[39;00m[38;5;21;01m.[39;00m[38;5;21;01mdatasets[39;00m [38;5;28;01mimport[39;00m RegressionDataset
[0;32m----> 4[0m [38;5;28;01mimport[39;00m [38;5;21;01mtempeh[39;00m[38;5;21;01m.[39;00m[38;5;21;01mconfigurations[39;00m [38;5;28;01mas[39;00m [38;5;21;01mtc[39;00m
[1;32m      7[0m [38;5;28;01mclass[39;00m [38;5;21;01mLawSchoolGPADataset[39;00m(RegressionDataset):
[1;32m      8[0m     [38;5;124;03m"""Law School GPA dataset.[39;00m
[1;32m      9[0m 
[1;32m     10[0m [38;5;124;03m    See https://github.com/microsoft/tempeh for details.[39;00m
[1;32m     11[0m [38;5;124;03m    """[39;00m

File [0;32m/opt/hostedtoolcache/Python/3.8.15/x64/lib/python3.8/site-packages/tempeh/configurations.py:6[0m
[1;32m      1[0m [38;5;66;03m# Copyright (c) Microsoft Corporation. All rights reserved.[39;00m
[1;32m      2[0m [38;5;66;03m# Licensed under the MIT License.[39;00m
[1;32m      4[0m [38;5;124;03m"""Holds all the datasets and models that are automatically enqueued."""[39;00m
[0;32m----> 6[0m [38;5;28;01mfrom[39;00m [38;5;21;01mtempeh[39;00m[38;5;21;01m.[39;00m[38;5;21;01mdatasets[39;00m [38;5;28;01mimport[39;00m SKLearnPerformanceDatasetWrapper, UCIPerformanceDatasetWrapper, \
[1;32m      7[0m     BlobPerformanceDatasetWrapper, CompasPerformanceDatasetWrapper, \
[1;32m      8[0m     SEAPHEPerformanceDatasetWrapper
[1;32m      9[0m [38;5;28;01mfrom[39;00m [38;5;21;01mtempeh[39;00m[38;5;21;01m.[39;00m[38;5;21;01mmodels[39;00m [38;5;28;01mimport[39;00m RBMSVMModelWrapper, LinearSVMModelWrapper, LogisticModelWrapper, \
[1;32m     10[0m     RidgeModelWrapper, DecisionTreeClassifierWrapper, RandomForestClassifierWrapper, \
[1;32m     11[0m     RandomForestRegressorWrapper, PytorchMulticlassClassifierWrapper, \
[1;32m     12[0m     PytorchBinaryClassifierWrapper, PytorchRegressionWrapper, XGBoostClassifierWrapper, \
[1;32m     13[0m     XGBoostRegressorWrapper, LightGBMClassifierWrapper, LightGBMRegressorWrapper, \
[1;32m     14[0m     KerasMulticlassClassifierWrapper, KerasBinaryClassifierWrapper, KerasRegressionWrapper
[1;32m     15[0m [38;5;28;01mfrom[39;00m [38;5;21;01mtempeh[39;00m[38;5;21;01m.[39;00m[38;5;21;01mconstants[39;00m [38;5;28;01mimport[39;00m DatasetSizes

File [0;32m/opt/hostedtoolcache/Python/3.8.15/x64/lib/python3.8/site-packages/tempeh/datasets/__init__.py:4[0m
[1;32m      1[0m [38;5;66;03m# Copyright (c) Microsoft Corporation. All rights reserved.[39;00m
[1;32m      2[0m [38;5;66;03m# Licensed under the MIT License.[39;00m
[0;32m----> 4[0m [38;5;28;01mfrom[39;00m [38;5;21;01m.[39;00m[38;5;21;01msk_datasets[39;00m [38;5;28;01mimport[39;00m SKLearnPerformanceDatasetWrapper
[1;32m      5[0m [38;5;28;01mfrom[39;00m [38;5;21;01m.[39;00m[38;5;21;01muci_datasets[39;00m [38;5;28;01mimport[39;00m UCIPerformanceDatasetWrapper
[1;32m      6[0m [38;5;28;01mfrom[39;00m [38;5;21;01m.[39;00m[38;5;21;01mblob_datasets[39;00m [38;5;28;01mimport[39;00m BlobPerformanceDatasetWrapper

File [0;32m/opt/hostedtoolcache/Python/3.8.15/x64/lib/python3.8/site-packages/tempeh/datasets/sk_datasets.py:12[0m
[1;32m      7[0m [38;5;28;01mfrom[39;00m [38;5;21;01m.[39;00m[38;5;21;01mbase_wrapper[39;00m [38;5;28;01mimport[39;00m BasePerformanceDatasetWrapper
[1;32m      9[0m [38;5;28;01mfrom[39;00m [38;5;21;01mtempeh[39;00m[38;5;21;01m.[39;00m[38;5;21;01mconstants[39;00m [38;5;28;01mimport[39;00m FeatureType, Tasks, DataTypes, SKLearnDatasets, ClassVars  [38;5;66;03m# noqa[39;00m
[0;32m---> 12[0m [38;5;28;01mclass[39;00m [38;5;21;01mSKLearnPerformanceDatasetWrapper[39;00m(BasePerformanceDatasetWrapper):
[1;32m     13[0m     [38;5;124;03m"""sklearn Datasets"""[39;00m
[1;32m     15[0m     dataset_map [38;5;241m=[39m {
[1;32m     16[0m         SKLearnDatasets[38;5;241m.[39mBOSTON: (datasets[38;5;241m.[39mload_boston, [FeatureType[38;5;241m.[39mCONTINUOUS] [38;5;241m*[39m [38;5;241m3[39m [38;5;241m+[39m [FeatureType[38;5;241m.[39mNOMINAL] [38;5;241m+[39m [FeatureType[38;5;241m.[39mCONTINUOUS] [38;5;241m*[39m [38;5;241m10[39m),  [38;5;66;03m# noqa: E501[39;00m
[1;32m     17[0m         SKLearnDatasets[38;5;241m.[39mIRIS: (datasets[38;5;241m.[39mload_iris, [FeatureType[38;5;241m.[39mCONTINUOUS] [38;5;241m*[39m [38;5;241m4[39m [38;5;241m+[39m [FeatureType[38;5;241m.[39mNOMINAL]),  [38;5;66;03m# noqa: E501[39;00m
[0;32m   (...)[0m
[1;32m     21[0m         SKLearnDatasets[38;5;241m.[39mCANCER: (datasets[38;5;241m.[39mload_breast_cancer, [FeatureType[38;5;241m.[39mCONTINUOUS] [38;5;241m*[39m [38;5;241m30[39m [38;5;241m+[39m [FeatureType[38;5;241m.[39mNOMINAL])  [38;5;66;03m# noqa: E501[39;00m
[1;32m     22[0m     }

File [0;32m/opt/hostedtoolcache/Python/3.8.15/x64/lib/python3.8/site-packages/tempeh/datasets/sk_datasets.py:16[0m, in [0;36mSKLearnPerformanceDatasetWrapper[0;34m()[0m
[1;32m     12[0m [38;5;28;01mclass[39;00m [38;5;21;01mSKLearnPerformanceDatasetWrapper[39;00m(BasePerformanceDatasetWrapper):
[1;32m     13[0m     [38;5;124;03m"""sklearn Datasets"""[39;00m
[1;32m     15[0m     dataset_map [38;5;241m=[39m {
[0;32m---> 16[0m         SKLearnDatasets[38;5;241m.[39mBOSTON: ([43mdatasets[49m[38;5;241;43m.[39;49m[43mload_boston[49m, [FeatureType[38;5;241m.[39mCONTINUOUS] [38;5;241m*[39m [38;5;241m3[39m [38;5;241m+[39m [FeatureType[38;5;241m.[39mNOMINAL] [38;5;241m+[39m [FeatureType[38;5;241m.[39mCONTINUOUS] [38;5;241m*[39m [38;5;241m10[39m),  [38;5;66;03m# noqa: E501[39;00m
[1;32m     17[0m         SKLearnDatasets[38;5;241m.[39mIRIS: (datasets[38;5;241m.[39mload_iris, [FeatureType[38;5;241m.[39mCONTINUOUS] [38;5;241m*[39m [38;5;241m4[39m [38;5;241m+[39m [FeatureType[38;5;241m.[39mNOMINAL]),  [38;5;66;03m# noqa: E501[39;00m
[1;32m     18[0m         SKLearnDatasets[38;5;241m.[39mDIABETES: (datasets[38;5;241m.[39mload_diabetes, [FeatureType[38;5;241m.[39mCONTINUOUS] [38;5;241m*[39m [38;5;241m11[39m),
[1;32m     19[0m         SKLearnDatasets[38;5;241m.[39mDIGITS: (datasets[38;5;241m.[39mload_digits, [FeatureType[38;5;241m.[39mCONTINUOUS] [38;5;241m*[39m [38;5;241m64[39m [38;5;241m+[39m [FeatureType[38;5;241m.[39mNOMINAL]),  [38;5;66;03m# noqa: E501[39;00m
[1;32m     20[0m         SKLearnDatasets[38;5;241m.[39mWINE: (datasets[38;5;241m.[39mload_wine, [FeatureType[38;5;241m.[39mCONTINUOUS] [38;5;241m*[39m [38;5;241m13[39m [38;5;241m+[39m [FeatureType[38;5;241m.[39mNOMINAL]),  [38;5;66;03m# noqa: E501[39;00m
[1;32m     21[0m         SKLearnDatasets[38;5;241m.[39mCANCER: (datasets[38;5;241m.[39mload_breast_cancer, [FeatureType[38;5;241m.[39mCONTINUOUS] [38;5;241m*[39m [38;5;241m30[39m [38;5;241m+[39m [FeatureType[38;5;241m.[39mNOMINAL])  [38;5;66;03m# noqa: E501[39;00m
[1;32m     22[0m     }
[1;32m     23[0m     metadata_map [38;5;241m=[39m {
[1;32m     24[0m         SKLearnDatasets[38;5;241m.[39mBOSTON: (Tasks[38;5;241m.[39mREGRESSION, DataTypes[38;5;241m.[39mTABULAR, ([38;5;241m506[39m, [38;5;241m13[39m)),
[1;32m     25[0m         SKLearnDatasets[38;5;241m.[39mIRIS: (Tasks[38;5;241m.[39mMULTICLASS, DataTypes[38;5;241m.[39mTABULAR, ([38;5;241m150[39m, [38;5;241m4[39m)),
[0;32m   (...)[0m
[1;32m     29[0m         SKLearnDatasets[38;5;241m.[39mCANCER: (Tasks[38;5;241m.[39mBINARY, DataTypes[38;5;241m.[39mTABULAR, ([38;5;241m569[39m, [38;5;241m30[39m))
[1;32m     30[0m     }
[1;32m     32[0m     load_function [38;5;241m=[39m [38;5;28;01mNone[39;00m

File [0;32m/opt/hostedtoolcache/Python/3.8.15/x64/lib/python3.8/site-packages/sklearn/datasets/__init__.py:156[0m, in [0;36m__getattr__[0;34m(name)[0m
[1;32m    105[0m [38;5;28;01mif[39;00m name [38;5;241m==[39m [38;5;124m"[39m[38;5;124mload_boston[39m[38;5;124m"[39m:
[1;32m    106[0m     msg [38;5;241m=[39m textwrap[38;5;241m.[39mdedent(
[1;32m    107[0m         [38;5;124;03m"""[39;00m
[1;32m    108[0m [38;5;124;03m        `load_boston` has been removed from scikit-learn since version 1.2.[39;00m
[0;32m   (...)[0m
[1;32m    154[0m [38;5;124;03m        """[39;00m
[1;32m    155[0m     )
[0;32m--> 156[0m     [38;5;28;01mraise[39;00m [38;5;167;01mImportError[39;00m(msg)
[1;32m    157[0m [38;5;28;01mtry[39;00m:
[1;32m    158[0m     [38;5;28;01mreturn[39;00m [38;5;28mglobals[39m()[name]

[0;31mImportError[0m: 
`load_boston` has been removed from scikit-learn since version 1.2.

The Boston housing prices dataset has an ethical problem: as
investigated in [1], the authors of this dataset engineered a
non-invertible variable "B" assuming that racial self-segregation had a
positive impact on house prices [2]. Furthermore the goal of the
research that led to the creation of this dataset was to study the
impact of air quality but it did not give adequate demonstration of the
validity of this assumption.

The scikit-learn maintainers therefore strongly discourage the use of
this dataset unless the purpose of the code is to study and educate
about ethical issues in data science and machine learning.

In this special case, you can fetch the dataset from the original
source::

    import pandas as pd
    import numpy as np

    data_url = "http://lib.stat.cmu.edu/datasets/boston"
    raw_df = pd.read_csv(data_url, sep="\s+", skiprows=22, header=None)
    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
    target = raw_df.values[1::2, 2]

Alternative datasets include the California housing dataset and the
Ames housing dataset. You can load the datasets as follows::

    from sklearn.datasets import fetch_california_housing
    housing = fetch_california_housing()

for the California housing dataset and::

    from sklearn.datasets import fetch_openml
    housing = fetch_openml(name="house_prices", as_frame=True)

for the Ames housing dataset.

[1] M Carlisle.
"Racist data destruction?"
<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>

[2] Harrison Jr, David, and Daniel L. Rubinfeld.
"Hedonic housing prices and the demand for clean air."
Journal of environmental economics and management 5.1 (1978): 81-102.
<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>

ImportError: 
`load_boston` has been removed from scikit-learn since version 1.2.

The Boston housing prices dataset has an ethical problem: as
investigated in [1], the authors of this dataset engineered a
non-invertible variable "B" assuming that racial self-segregation had a
positive impact on house prices [2]. Furthermore the goal of the
research that led to the creation of this dataset was to study the
impact of air quality but it did not give adequate demonstration of the
validity of this assumption.

The scikit-learn maintainers therefore strongly discourage the use of
this dataset unless the purpose of the code is to study and educate
about ethical issues in data science and machine learning.

In this special case, you can fetch the dataset from the original
source::

    import pandas as pd
    import numpy as np

    data_url = "http://lib.stat.cmu.edu/datasets/boston"
    raw_df = pd.read_csv(data_url, sep="\s+", skiprows=22, header=None)
    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
    target = raw_df.values[1::2, 2]

Alternative datasets include the California housing dataset and the
Ames housing dataset. You can load the datasets as follows::

    from sklearn.datasets import fetch_california_housing
    housing = fetch_california_housing()

for the California housing dataset and::

    from sklearn.datasets import fetch_openml
    housing = fetch_openml(name="house_prices", as_frame=True)

for the Ames housing dataset.

[1] M Carlisle.
"Racist data destruction?"
<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>

[2] Harrison Jr, David, and Daniel L. Rubinfeld.
"Hedonic housing prices and the demand for clean air."
Journal of environmental economics and management 5.1 (1978): 81-102.
<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>


